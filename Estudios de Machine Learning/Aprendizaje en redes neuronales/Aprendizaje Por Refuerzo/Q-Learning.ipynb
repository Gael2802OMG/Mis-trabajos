{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el Reinforcement Learning?\n",
    "    + Diferencias con los clasicos\n",
    "    + Componentes  \n",
    "2. Casos de Uso  \n",
    "    + Videojuegos\n",
    "3. Cómo funciona el RL?\n",
    "    + Premios y castigos\n",
    "    + Fuerza bruta\n",
    "4. Q-Learnign \n",
    "    + Ecuación de Bellman\n",
    "    + Explorar vs Explotar\n",
    "5. El juego del Pong en python\n",
    "    + Clase Agente\n",
    "    + Clase Entorno\n",
    "    + El Juego\n",
    "    + La tabla de Políticas\n",
    "6. Conclusiones\n",
    "    + Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es el Aprendizaje por Refuerzo?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un aprendizaje de maquina en el que se basa en un esquema de \"premios y castigos\" en un <b>entorno</b> donde hay que tomar acciones y que está afectado por múltiples variables que cambian con el tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencias\n",
    "\n",
    "Los modelos anteriores de aprendizaje supervisado y no supervisado, su forma de aprendizaje es crear un modelo que minimise el error obtenido y corregirlos de forma recursiva. Por otro lado el aprendizaje reforzado <b>Maximiza la recompenza</b>. Por lo que cambia la forma de aprendizaje, siendo más parecido a la forma en la que nosotros aprendemos, a base de cumplir objetivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes de Aprendizaje Reforzado \n",
    "+ <b>Agente:</b> Modelo de entranamiento que aprenderá a tomar desiciones\n",
    "+ <b>Ambiente:</b> Será el entorno en donde interactúa el agente  \n",
    "##### Relación entre el Agente y el Entorno\n",
    "+ <b>Acción:</b> Posibles acciones que puede tomar el agente en base a lo que perciba del entorno\n",
    "+ <b>Estado:</b> La percepción que transmite el entorno de los elementos que lo componen, en cada segundo\n",
    "+ <b>Recompensas y Castigos:</b> Depende de la acción tomada por el Agente, y puede dar resultados positivos (+1) o negativos (-1) siendo estos premios ó penalizaciones respectivamente.  \n",
    "\n",
    "La relación que tienen el Agente y el Ambiente quedaría de la siguiente manera:  \n",
    "<br>\n",
    "<img src=\"RL1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de Uso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><b>Brazos Mecanicos:</b></li> En vez de darles instrucciones paso a paso, se realiza RL e ir recompensandolo para guiarlo a las metas que queremos que consiga\n",
    "    <br><br>\n",
    "    <li><b>Industrial / Mercado:</b></li> Se puede hacer que el Agente interactué en situaciones industriales y financieras del mundo real, dando mantenimiento a maquinas o decidir entre una cartera de inversiones\n",
    "    <br><br>\n",
    "    <li><b>Web Personalizada:</b></li> Todos los usuarios tienen gustos diferentes y sería completamente imposible crear un algoritmo que le dé a cada usuario referencias de busqueda en base a lo que cada quien le interesa en ese momento especifico, por que RL abre las puertas para personalizar la experiencia de cada persona que navege por la web \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Elementos </h4>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Politicas:</b></li> Una tabla de indicaciones para decirle al modelo como actuar en cada estado.\n",
    "    <li><b>Acciones:</b></li> Elecciones que puede tomar el agente dependiendo del estado del entorno.\n",
    "    <li><b>Recompensas:</b></li> Suma y resta de puntajes dados por el entorno al agente.\n",
    "    <li><b>Comportamiento</b></li> Supervisar si sus acciones se quedan estancadas en grandes recompenzas sin completar el objetivo o seguirá explorando otros objetivos a largo plazo.\n",
    "    <li><b>Q(Estado / Acción):</b></li> nos indicará el valor de la política para un estado y una cción determinados.\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Bellman.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) = Q(s,a) + a[R+((\\gamma)maxQ(s',))-Q(s,a)]$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡A Jugar al Ping Pong! :3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Agente:</b>\n",
    "+ Nombre: Player 1\n",
    "+ Acciones:\n",
    "    + moverse arriba\n",
    "    + moverse abajo\n",
    "\n",
    "<b>Reglas del juego:</b>\n",
    "+ El agente tiene 3 vidas.\n",
    "+ si pierde le resta 10 puntos\n",
    "+ cada vez que le demos la bola, gana 10 puntos\n",
    "+ Limite de juego: 3000 iteraciones o 1000 puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b> Igualaciones de la ecuación a variables dentro del Agente </b>\n",
    "\n",
    "+ $Q(s,a)$ = Valor Actual => actualValue\n",
    "+ $a$ = Ratio de Aprendizaje => LearningRatio\n",
    "+ $R$ = Recompenza => rewardAction\n",
    "+ $\\gamma$ = Tasa de descuento => descontador\n",
    "+ $maxQ(s',)$ = Valor optimo Esperado => idxAction (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos Liberías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from math import ceil,floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clase Agente\n",
    "class PongAgent:\n",
    "    def __init__(self, game, policy=None, descontador = 0.1, learningRatio = 0.1, ratioExplot = 0.9):\n",
    "\n",
    "        # Creamos la tabla de politicas\n",
    "        if policy is not None:\n",
    "            self._q_table=policy\n",
    "        else:\n",
    "            position = list(game.posicionSpace.shape) \n",
    "            # Transifere las dimensiones del entorno para que el Agente empiece a percibir\n",
    "            # donde se encuentra\n",
    "            position.append(len(game.actionSpace))\n",
    "            # agrega cuantas acciones tiene el entorno\n",
    "            self._q_table = np.zeros(position)\n",
    "            # Crea un array de zeros con respecto a lo que se obtuvo anteriormente\n",
    "            # Tabla de politicas -> Creada/Inicializada\n",
    "        #############################################\n",
    "\n",
    "        self.descontador = descontador\n",
    "        self.learingRatio = learningRatio\n",
    "        self.RatioExplot = ratioExplot\n",
    "\n",
    "    def getNextStep(self, state, game):\n",
    "\n",
    "        # Damos un paso aleatorio...\n",
    "        nextStep = np.random.choice(list(game.actionSpace))\n",
    "        # Obtiene una lista de acciones sobre el entorno\n",
    "        # De esta lista se selecciona una acción de forma aleatoria\n",
    "        #------------------------------------------------------------#\n",
    "        # o tomaremos el mejor paso...\n",
    "        if np.random.uniform() <= self.RatioExplot:\n",
    "            # Se selecciona de forma aleatoria un valor de entre 0 y 1\n",
    "            # siendo este valor <= al radio de explotación del agente\n",
    "            #---------------------------------------------------------#\n",
    "            # tomar el maximo\n",
    "            idxAction = np.random.choice(np.flatnonzero(\n",
    "                self._q_table[state[0],state[1],state[2]] == self._q_table[state[0],state[1],state[2]].max()  \n",
    "            ))\n",
    "            # Busca entre todos los elementos de la tabla de politicas, numeros != 0 y \n",
    "            # los remplaza unicamente por los valores maximos de su propia tabla de politicas\n",
    "            #-----------------------------------------------------------------------------------------------#\n",
    "            nextStep = list(game.actionSpace)[idxAction]\n",
    "            # El siguiente paso es igualado a una lista de acciones del entorno\n",
    "            # y el valor maximo de la tabla de politicas\n",
    "\n",
    "        return nextStep\n",
    "    \n",
    "    # actualizamos las politicas con las recompensas obtenidas\n",
    "    def update(self, game, oldState, actionTaken, rewardAction, newState, reachedEnd):\n",
    "        idxAction = list(game.actionSpace).index(actionTaken)\n",
    "        # la acción es igualada a las posibles acciones y la acción tomada con anterioridad\n",
    "\n",
    "        actualValueOpt = self._q_table[oldState[0],oldState[1],oldState[2]]\n",
    "        # El valor opcional actual toma un estado de la tabla de politicas \n",
    "        # en funcion de un estado anterior\n",
    "        actualValue = actualValueOpt[idxAction]\n",
    "        # Guarda el valor actual directamente de la tabla de estados anterior \n",
    "        # para obtener la acción tomada del estado anterior\n",
    "        futureValueOpt = self._q_table[newState[0],newState[1],newState[2]]\n",
    "        # Consigue valores actualizados de la tabla de politicas con estados nuevos\n",
    "        futureMaxValue = rewardAction + self.discountFactor * futureValueOpt.max()\n",
    "        # Obtiene un posible valor en función a la recompenza obtenida\n",
    "        if reachedEnd:\n",
    "            futureMaxValue = rewardAction\n",
    "            # Si llega al final obtiene directamente la recompenza\n",
    "        \n",
    "        self._q_table[oldState[0], oldState[1], oldState[2], idxAction] = actualValue + \\\n",
    "        self.learningRate * (futureMaxValue - actualValue)\n",
    "\n",
    "        # Actualiza la tabla de politicas\n",
    "    \n",
    "    def printPolicy(self):\n",
    "        for row in np.round(self._q_table,1):\n",
    "            for column in row:\n",
    "                print('[', end='')\n",
    "                for value in column:\n",
    "                    print(str(value).zfill(5), end='')\n",
    "                print('] ',end='')\n",
    "            print('')\n",
    "\n",
    "    def getPolicy(self):\n",
    "        return self._q_table\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clase Entorno\n",
    "class PongEnvironment:\n",
    "    def __init__(self, maxLife=3,heightPx=40,widthPx=50,movimientoPx=3):\n",
    "        self.actionSpace=['Arriba','Abajo']\n",
    "        self.stepPenalization=0\n",
    "        self.state=[0,0,0]\n",
    "        self.totalReward=0\n",
    "        self.dx=movimientoPx\n",
    "        self.dy=movimientoPx\n",
    "        \n",
    "        filas = ceil(heightPx/movimientoPx)\n",
    "        columnas = ceil(widthPx/movimientoPx)\n",
    "\n",
    "        self.positionSpace = np.array([[[0 for z in range(columnas)]\n",
    "                                                for y in range(filas)]\n",
    "                                                    for x in range(filas)])\n",
    "        self.lives=maxLife\n",
    "        self.maxLife=maxLife\n",
    "        self.x=randint(int(widthPx/2),widthPx)\n",
    "        self.y=randint(0,heightPx-10)\n",
    "        self.playerAlto = int(heightPx/4)\n",
    "        self.player1 = self.playerAlto # posición inicial del jugador\n",
    "        self.score = 0\n",
    "        self.widthPx=widthPx\n",
    "        self.heightPx=heightPx\n",
    "        self.radio=2.5\n",
    "    \n",
    "    def reset(self):\n",
    "        self.totalReward=0\n",
    "        self.state=[0,0,0]\n",
    "        self.lives=self.maxLife\n",
    "        self.score=0\n",
    "        self.x=randint(int(self.widthPx/2),self.widthPx)\n",
    "        self.y=randint(0,self.heightPx-10)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def step(self,action,animate=False):\n",
    "        self.applyAction(action,animate)\n",
    "        done=self.lives<=0 #Final\n",
    "        reward = self.score\n",
    "        reward += self.stepPenalization\n",
    "        self.totalReward+=reward\n",
    "        return self.state,reward,done\n",
    "    \n",
    "    def applyAction(self,action,animate=False):\n",
    "        if action == 'Arriba':\n",
    "            self.player1 += abs(self.dy)\n",
    "        elif action == 'Abajo':\n",
    "            self.player1 -= abs(self.dy)\n",
    "        \n",
    "        self.avanzaPlayer()\n",
    "        \n",
    "    def avanzaPlayer(self):    \n",
    "        if self.player1 + self.playerAlto >= self.heightPx:\n",
    "            self.player1 = self.heightPx - self.playerAlto\n",
    "        elif self.player1 <= -abs(self.dy):\n",
    "            self.player1 = -abs(self.dy)\n",
    "    \n",
    "    def avanzaFrame(self):\n",
    "        self.x+=self.dx\n",
    "        self.y+=self.dy\n",
    "        if self.x <= 3 or self.x > self.widthPx:\n",
    "            self.dx = -self.dx\n",
    "            if self.x <= 3:\n",
    "                ret = self.detectaColision(self.y,self.player1)\n",
    "\n",
    "                if ret:\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.score = -10\n",
    "                    self.lives -= 1\n",
    "                    if self.lives > 0:\n",
    "                        self.x = randint(int(self.widthPx/2),self.widthPx)\n",
    "                        self.y = randint(0, self.heightPx-10)\n",
    "                        self.dx = abs(self.dx)\n",
    "                        self.dy = abs(self.dy)\n",
    "        else:\n",
    "            self.score=0\n",
    "        \n",
    "        if self.y < 0 or self.y > self.heightPx:\n",
    "            self.dy = -self.dy\n",
    "    \n",
    "    def dibujarFrame(self):\n",
    "        fig = plt.figure(figsize=(5,4))\n",
    "        a1 = plt.gca()\n",
    "        circle = plt.Circle((self.x,self.y),self.radio,fc='slategray',ec='black')\n",
    "        a1.set_ylim(-5,self.heightPx+5)\n",
    "        a1.set_xlim(-5,self.widthPx+5)\n",
    "                \n",
    "        rectangle = plt.Rectangle((-5,self.player1),5,self.playerAlto,fc='gold',ec='none')\n",
    "        a1.add_patch(circle)\n",
    "        a1.add_patch(rectangle)\n",
    "        a1.set_yticklabels([])\n",
    "        a1.set_xticklabels([])\n",
    "        plt.text(4,self.heightPx,\"SCORE:\"+str(self.totalReward)+\" LIFE:\"+str(self.lives),fontsize=12)\n",
    "        if self.lives <= 0:\n",
    "            plt.text(10,self.heightPx-14, \"GAME OVER\", fontsize=16)\n",
    "        elif self.totalReward >= 1000:\n",
    "            plt.text(10,self.heightPx-14, \"YOU WIN\", fontsize=16)\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el Juego\n",
    "def play(rounds = 5000, maxLife = 3, discountFactor = 0.1, learningRatio=0.1,\n",
    "         ratioExplot = 0.9, learner= None, game=None, animate = False):\n",
    "    if game is None:\n",
    "        game = PongEnvironment(maxLife=maxLife,movimientoPx=3)\n",
    "    if learner is None:\n",
    "        print(\"Begin New Train!\")\n",
    "        learner = PongAgent(game, discountFactor = discountFactor, learningRatio = learningRatio, ratioExplot=ratioExplot)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
